\documentclass[11pt,a4paper]{amsart}

\usepackage{amssymb}
\usepackage{geometry}
\usepackage{commath}
\usepackage{dsfont}
\usepackage{enumerate}
\usepackage{graphicx}
\usepackage{todonotes}
%\setlength{\parindent}{0pt}

\newtheorem*{problem}{Problem}
\theoremstyle{definition}
\newtheorem*{answer}{Answer}
\DeclareMathOperator{\supp}{supp}
\DeclareMathOperator{\ran}{ran}
\DeclareMathOperator{\dom}{dom}
\DeclareMathOperator{\spann}{span}
\DeclareMathOperator{\col}{col}
\DeclareMathOperator{\row}{row}
\DeclareMathOperator{\Id}{Id}
\renewcommand{\span}{\spann}
\newcommand{\bbone}{\mathds{1}}
\begin{document}

\title{A first implementation of Section 3 of ``Bla Parabolic''}
\author{Jan Westerdiep}
\maketitle

I've spent the past two weeks on a (not yet) linear time implementation of \S 3
from ``Bla Parabolic''. Currently it can only apply multiscale (mass) operator
in the time direction; I felt it was better to have a good understanding of one
aspect than to produce a half-baked implementation for the tensor-product case.

\subsection*{A little bit on notation} I am somewhat hesitant to call these
`mass matrices' \emph{matrices} because this implies that the rows and columns
are ordered, whereas we have mere \emph{sets} of indices (which are not necessarily
ordered). I will try to call everything \emph{linear operators} from one
\emph{vector space} to another, instead of matrices working on vectors.

The vector spaces that we will be working with are often sequence spaces
$\ell_2(\Lambda)$ for some finite index set $\Lambda$. If we were to order this
index set in, say, lexicographic ordering, then $\ell_2(\Lambda)$ corresponds
directly with $\mathbb{R}^{\# \Lambda}$.

\section{The Applicator}
The meat of the algorithm is in \texttt{applicator.py}. This \texttt{Applicator}
class has a constructor which takes, at minimum, a \texttt{Basis} object
$B = \Phi \cup \Psi$ and a singlescale \texttt{LinearOperator} object
$\mathcal A: \span \Phi \to (\span \Phi)'$. Both $\Phi$ and $\Psi$ are indexed by
\texttt{IndexSets}, with $\Phi$ indexed by a list of \texttt{SingleLevelIndexSet}s
$\Delta_\ell$ and $\Psi$ by a \texttt{MultiScaleIndexSet} (or \emph{tree})
which we call $\Lambda$. Remember that by assumption $\span \Phi = \span \Psi$.
It is also possible to construct the \texttt{Applicator} with different in- and
out-bases when the domain and range of $\mathcal A$ do not coincide.

It has three public methods: \texttt{apply()}, \texttt{apply\_upp()},
and \texttt{apply\_low()}. These methods in turn invoke recursive methods that
traverse $\Lambda$, starting from level $\ell=1$ until its maximum level, at
each level applying $\mathcal A$ to a subset of the singlescale indices
$\Delta_\ell$ on level $\ell$. In doing this, it needs the \texttt{SLIS}s
$\check \Pi_B^\ell := \check \Pi_B$ (named \texttt{Pi\_B\_out} in the code
corresponding with the out-basis), $\Pi_B^\ell = \Pi_B$ named
\texttt{Pi\_B\_in}, $\underline \Pi^\ell = \Pi^{\ell + 1}$ named \texttt{Pi\_bar\_in},
and $\underline{\check \Pi}^\ell = \check \Pi^{\ell+1}$. It is important to note 
that even when $\ran \mathcal A = \dom \mathcal A$, in general $\check \Pi_B \not= \Pi_B$
and hence $\underline{\check \Pi} \not= \underline \Pi$.

Computing these subsets $\check \Pi_B, \Pi_B, \underline{\check \Pi}, \underline \Pi$
in linear time is not an easy task. I was able to produce them in suboptimal complexity;
see the code. Maybe Rob will have an idea on how to tackle these.

\subsection{Constructing $\Pi_B$}
I am not \emph{at all} sure what $\Pi_B$ is supposed to do, but I feel like it's
the set of indices that are supposed to be lifted to level $\ell+1$. I computed 
\[
  \Pi_B := \set{\lambda \in \Pi: |\supp \phi_\lambda \cap
      (\cup_{\mu \in \check \Lambda_\ell} \supp \check \psi_\mu
       \cup_{\gamma \in \check \Pi_B} \supp \check \phi_\gamma)| > 0}
\]
by collecting all these supports $[a,b]$, sorting them\footnote{Blehhhh; see code
for a justification and a possible solution; still, this doesn't reduce the
operation to optimal complexity.} in order of increasing $a$, and merging them
pairwise to a minimum-size \texttt{IntervalSet} that covers the same intervals
as the original supports. We can loop over all $\lambda \in \Pi$ and check if
this \texttt{IntervalSet} \texttt{intersects()} its support. This is possibly
$\mathcal O[\# \Pi_B (\# \Lambda_\ell + \# \check \Pi_B) \log(\# \Lambda_\ell + \# \check \Pi_B)]$
operations where we would like $\mathcal O(\# \Pi_B$).  In practice though,
$\#\mathtt{IntervalSet} \ll \# \Lambda_\ell + \# \check \Pi_B$, so this helps
\emph{a lot} for faster runtimes.

On the other
hand $\Pi_A := \Pi \setminus \Pi_B$ is the set of indices we will want restrict
the result of applying $\mathcal A$ to, and these will not be lifted to the next
level. In Kestlers C++-code, (the equivalent of) $\Pi_A$ rather than $\Pi_B$ is
computed. This may be a way of doing it in linear time?

\subsection{Constructing $\underline \Pi$}
The construction of $\underline \Pi$ is very similar to the previous method, the
big difference being that we want to collect those indices $\lambda \in \Delta_\ell$
that are completely covered by the \texttt{IntervalSet}.

\subsection{Applying $P^\ell$ and $Q^\ell$, and restriction of vectors to subsets}
Throughout the algorithm, it is necessary to restrict a vector to a subset of
indices, e.g.
\[
  \underline{\vec d} := (P^\ell \vec d|_{\Pi_B} + Q^\ell \vec c|_{\Lambda_\ell})|_{\underline \Pi}.
\]
Instead of manually setting vector indices to zero, computing the result of the
linear operator on the full set $\Delta_{\ell + 1}$ and then again setting vector
indices to zero, instead we compute the result of the \texttt{LinearOperator}s
$P^\ell$ and $Q^\ell$ only on those indices in $\underline \Pi$, and in the inner
products of rows of the \texttt{LinearOperator} with the \texttt{IndexedVector},
we only consider those indices in $\Pi_B$ resp.~$\Lambda_\ell$, thus effectively
`zero-ing out' the other indices. We know that every
row of $P^\ell$ and $Q^\ell$ have $\lesssim 1$ number of nonzeros, so this inner
product can be computed in $O(1)$ operations.\footnote{One actually has to take
care in not looping over elements in $\Pi_B$ to see if the corresponding cell in
$P^\ell$ is nonzero, but rather loop over nonzeros in $P^\ell$ to see if they
are in $\Pi_B$. This mistake destroyed linearity at first, and took me quite some
time to fix.}

The linear operators $P^\ell$ and $Q^\ell$ are the unique ones such that
\[
  \Phi_{\ell-1}^\top = \Phi_\ell^\top P^\ell, \quad \Psi_\ell^\top = \Phi_\ell^\top Q^\ell.
\]
These exist because $\span \Phi_{\ell-1} \subset \span \Phi_\ell$ and
$\span \Psi_\ell \subset \span \Phi_\ell$ by assumption. For some $\phi_\lambda \in \Phi_{\ell-1}$,
we see that $\Phi_\ell^\top p_{\ell,\lambda}$: the linear operator $P^\ell$ gives us the
coefficients necessary to build a scaling function on the previous level using
scaling functions on this level: a little more on this later. Similar equalities
hold for the $Q^\ell$.

We also have to apply the transpose of $P^\ell$ and $Q^\ell$; building these
efficiently proved to be quite the obstacle for the more esoteric 3-point basis.

\subsection{Summation of vectors}
There is a number of places in the \texttt{Applicator} where vectors are built
by stacking multiple vectors indexed by disjoint sets (e.g.~$\vec e$ and $\vec f$).
We can be lazy and just write these as the sum of the two vectors.

\section{Bases}
In this proof-of-concept application, we will work with three bases: the Haar
basis, the orthonormal discontinuous piecewise linear basis, and the three-point
hierarchical basis. Every basis contains a \emph{singlescale} or \emph{scaling
basis} $\Phi$ indexed on every level by $\Delta_\ell$, and a \emph{multiscale}
or \emph{wavelet basis} $\Psi$ indexed by a tree $\Lambda$. Every index $lambda$
is a tuple $(\ell, n)$; its level and its \emph{offset}, even though in the
multiwavelet/multiscaling case, the offset also determines \emph{which} mother
scaling function $\phi_\lambda$ descends from.

Every \texttt{Basis} object is constructed by passing it the \texttt{MultiScaleIndexSet}
$\Lambda$. Upon construction, it exposes as properties the \texttt{LinearOperator}s
$P := (P^\ell)_{\ell}$ and $Q = (Q^\ell)_\ell$, as well as one or more singlescale
\texttt{LinearOperator}s like the mass and \emph{damping} operators
\[
  (M\Phi_\ell)(\Phi_\ell) := \left[\int_0^1 \phi_\lambda \phi_\mu \dif t \right]_{\lambda,\mu \in \Delta_\ell}, \quad
  (C\Phi_\ell)(\Phi_\ell) := \left[\int_0^1 \phi_\lambda' \phi_\mu \dif t \right]_{\lambda,\mu \in \Delta_\ell}.
\]

It has methods to compute the support of scaling- and wavelet functions (with
\texttt{Interval} return types), as well as the \emph{wavelet neighbourhood}
$S(\mu) \supset \supp \psi_\mu$ (which, for all our cases, just equals the support).

\subsection{Singlescale matrices}
I basically just computed the singlescale matrices using pen and paper or
Mathematica.

\subsection{$P^\ell$ and $Q^\ell$}
In general, the following equalities define $P^\ell$ and $Q^\ell$:
\[
\begin{cases}
  \phi_\lambda := \langle\Phi_\ell, \col_\lambda(P^\ell)\rangle_{\ell_2(\Delta_\ell)} = \sum_{\mu \in \Delta_\ell} P^\ell_{\mu,\lambda} \phi_\mu & \lambda \in \Delta_{\ell-1},\\
  \psi_\lambda := \langle \Phi_{\ell}, \col_\lambda(Q^\ell)\rangle_{\ell_2(\Delta_\ell)} = \sum_{\mu \in \Delta_\ell} Q^\ell_{\mu,\lambda} \phi_\mu & \lambda \in \Lambda_\ell.
\end{cases}
\]
This means that $P^\ell$ tells us exactly how to write a scaling function on the
previous level in terms of scaling functions on the current level, and that $Q^\ell$
tells us how to write wavelet functions on this level in terms of scaling functions
on this level.

The Haar basis has mother scaling resp.~wavelet functions
\[
  \phi(x) := \bbone_{[0,1)}(x), \quad \psi(x) := \bbone_{[0,\frac{1}{2})}(x) - \bbone_{[\frac{1}{2},1)}(x).
\]
At index $\lambda = (\ell, n)$, we have
\[
  \phi_\lambda(x) = \phi(2^\ell x - n), \quad \psi_\lambda(x) = \psi(2^{\ell-1}x - n).
\]
This $2^\ell$ versus $2^{\ell-1}$ is to balance the assumption that on level 0,
$\Psi_0 = \Phi_0$ (so that the mother wavelet function actually lives on level 1,
whereas the mother scaling function lives on level 0).

\subsubsection*{Haar basis}
The result is that for the Haar basis, both $P$ and $Q$ have very simple structure:
\[
  \phi_\lambda = \phi_{(\ell+1, 2n)} + \phi_{(\ell+1, 2n+1)}, \quad
  \psi_\lambda = \phi_{(\ell, 2n)} - \phi_{(\ell, 2n+1)}.
\]
In other words, every column of $P^\ell$ and $Q^\ell$ has two nonzeros. If we
were to order the indices in lexicographic ordering, then
\[
  P^{\ell} = \begin{bmatrix}1 \\ 1\end{bmatrix} \otimes \Id_{\ell}, \quad Q^{\ell} = \begin{bmatrix}1 \\ -1\end{bmatrix} \otimes \Id_{\ell}.
\]

\subsubsection*{Orthonormal basis}
The orthonormal discontinuous piecewise linear basis is a multiscaling/multiwavelet basis and has mother scaling functions
\[
  \phi_0(x) := \bbone_{[0,1)}(x), \quad \phi_1(x) := \sqrt{3} (2x-1) \bbone_{[0,1)}(x)
\]
with mother wavelet functions
\[
  \psi_0(x) := (1-6x) \bbone_{[0,\frac{1}{2})}(x) + (5-6x)\bbone_{[\frac{1}{2},1)}(x), \quad
  \psi_1(x) := \sqrt{3} (1-4x) \bbone_{[0,\frac{1}{2})}(x) + \sqrt{3}(4x-3)\bbone_{[\frac{1}{2},1)}(x)
\]
(see also the document by Rob for a more visual approach). We have
\[
  \phi_{(\ell, 2n)}(x) := \phi_0(2^\ell x - n), \quad \phi_{(\ell, 2n+1)}(x) := \phi_1(2^\ell x - n) \quad (\ell \geq 0, 0 \leq n < 2^{\ell+1})
\]
and
\[
  \psi_{(0, n)}(x) := \phi_n(x), \quad
  \psi_{(\ell, 2n)}(x) := 2^{(\ell-1)/2} \psi_0(2^{\ell-1} x - n), \quad
  \psi_{(\ell, 2n+1)}(x) := 2^{(\ell-1)/2} \psi_1(2^{\ell-1} x - n).
\]
This rescaling ensures that the multiscale mass operator is just the identity!
(Hence the name of the basis.) For this basis, the matrices $P^\ell$ and $Q^\ell$
are already a lot less obvious.\todo{somehow show in a figure how I derived them}

\subsubsection*{3-point basis}
The 3-point basis was introduced very tersely in the document, and I was not entirely
sure what Rob meant with the figures. I digged around and found a 1997 paper entitled
\emph{Experiments in 3D with a three-point hierarchical basis}, which seemed to
discuss the same basis.

First, let's consider the indices only. At level $\ell \geq 0$, a \emph{singlescale}
index can be any $\lambda = (\ell, n)$ with $0 \leq n \leq 2^\ell$, and
corresponds with a node at position $n/2^\ell$. A \emph{multiscale} index
$\lambda = (\ell, n)$ at level $\ell \geq 1$ has $0 \leq n \leq 2^{\ell-1}$ and
corresponds with a node at position $2^{-\ell} + 2^{1-\ell} n$. (Draw this out
for yourself on a piece of paper!) At level $\ell=0$, multiscale indices are
$(0,0)$ and $(0,1)$ at positions $0$ and $1$, respectively.

For $\ell \geq 2$, the \emph{parent} of a multiscale index $\lambda$ is the
unique multiscale index on level $\ell-1$ such that its position is adjacent to
$\lambda$ on the grid. The assumption that our multiscale index set $\Lambda$ is
a \emph{tree} in this setting coincides with the assumption that for any $\lambda$
with $\ell\geq 2$, its parent is also in $\Lambda$.

Let's take some $\Lambda$. Define $\Lambda_\ell := \set{\lambda \in \Lambda: |\lambda| = \ell}$ as the multiscale index sets on level $\ell$. By assumption,
$\Lambda_0 = \set{(0,0), (0,1)}$ (nodes at $0$ and $1$), and
$\Lambda_1 = \set{(1,0)}$ (single node at $\frac{1}{2}$). Then 
\[
  \begin{cases}
    \Lambda_2 \subset \set{(2,0), (2,1)} &\text{ nodes at } \tfrac{1}{4}, \tfrac{3}{4}, \\
    \Lambda_3 \subset \set{(3,0), (3,1), (3,2), (3,3)} &\text{ nodes at } \tfrac{1}{8}, \tfrac{3}{8}, \tfrac{5}{8}, \tfrac{7}{8}, \quad \text{etc.}
  \end{cases}
\]
Taking all multiscale indices until level $\ell$, we can define the set
$\Delta_\ell$ of singlescale indices that have the same node positions.
The idea is that the scaling functions on level $\ell$ are just hat functions
(rescaled with factor $2^{\ell/2}$) wrt the nodes of $\Delta_\ell$,\footnote{This
is different from what is sketched in figure 1 of the document.} and the
wavelet functions are linear combinations of (at most) three scaling functions.
\todo{show a couple of example $\Lambda$'s, $\Phi$'s and $\Psi$'s.}

\section{Tests}
I tried to test most essential parts of the code:
\begin{enumerate}
  \item Do the matrices $P^\ell$ and $Q^\ell$ work, i.e.~do we recover $\phi_\lambda$
    through $\langle \Phi_\ell, \col_\lambda(P^\ell) \rangle$?
  \item Are our multiscale mass matrices diagonal (Haar)/the identity (orthonormal)?
  \item Does the output of \texttt{apply()} correspond with values found through quadrature?
  \item Does the output of \texttt{apply()} equal \texttt{apply\_upp() + apply\_low()}?
\end{enumerate}
You can try running \texttt{pytest} in this folder, and it should pass all tests.

\section{Numerics}
\subsection{Bottlenecks}

\section{Towards tensor-product operators}

\end{document}
